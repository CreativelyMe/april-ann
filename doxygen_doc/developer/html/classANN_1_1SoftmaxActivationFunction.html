<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8">
<title>april-ann</title>
<link href="our_doxygen.css" rel="stylesheet" type="text/css">
<link href="tabs.css" rel="stylesheet" type="text/css">
</head><body>
<div class="tabs"><ul>
<li><a href="../../user_refman/html/index.html"><span>april-ann (user reference manual)</span></a></li>
<li id="current"><a href="../../developer/html/index.html"><span>april-ann (developer manual)</span></a></li>
</ul></div>
<!-- Generated by Doxygen 1.8.1.2 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="inherits.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespaceANN.html">ANN</a></li><li class="navelem"><a class="el" href="classANN_1_1SoftmaxActivationFunction.html">SoftmaxActivationFunction</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pri-attribs">Private Attributes</a> &#124;
<a href="classANN_1_1SoftmaxActivationFunction-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">ANN::SoftmaxActivationFunction Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>A softmax activation function.  
 <a href="classANN_1_1SoftmaxActivationFunction.html#details">More...</a></p>
<div class="dynheader">
Inheritance diagram for ANN::SoftmaxActivationFunction:</div>
<div class="dyncontent">
<div class="center"><img src="classANN_1_1SoftmaxActivationFunction__inherit__graph.png" border="0" usemap="#ANN_1_1SoftmaxActivationFunction_inherit__map" alt="Inheritance graph"/></div>
<map name="ANN_1_1SoftmaxActivationFunction_inherit__map" id="ANN_1_1SoftmaxActivationFunction_inherit__map">
<area shape="rect" id="node2" href="classANN_1_1ActivationFunction.html" title="ANN::ActivationFunction" alt="" coords="31,83,196,112"/><area shape="rect" id="node4" href="classReferenced.html" title="Referenced" alt="" coords="69,5,157,35"/></map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for ANN::SoftmaxActivationFunction:</div>
<div class="dyncontent">
<div class="center"><img src="classANN_1_1SoftmaxActivationFunction__coll__graph.png" border="0" usemap="#ANN_1_1SoftmaxActivationFunction_coll__map" alt="Collaboration graph"/></div>
<map name="ANN_1_1SoftmaxActivationFunction_coll__map" id="ANN_1_1SoftmaxActivationFunction_coll__map">
<area shape="rect" id="node2" href="classANN_1_1ActivationFunction.html" title="ANN::ActivationFunction" alt="" coords="5,85,171,115"/><area shape="rect" id="node4" href="classReferenced.html" title="Referenced" alt="" coords="44,5,132,35"/><area shape="rect" id="node6" href="classGPUMirroredMemoryBlock.html" title="GPUMirroredMemoryBlock\&lt; T \&gt;" alt="" coords="195,85,405,115"/></map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a45abc5e6e6ff36d60d3221be6337ae71"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a45abc5e6e6ff36d60d3221be6337ae71">SoftmaxActivationFunction</a> ()</td></tr>
<tr class="memitem:a212d8e80f5494a443b184f9f150116b4"><td class="memItemLeft" align="right" valign="top">virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a212d8e80f5494a443b184f9f150116b4">~SoftmaxActivationFunction</a> ()</td></tr>
<tr class="memitem:ad9116c5cdfeab37e2320811fb3a71324"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classANN_1_1SoftmaxActivationFunction.html#ad9116c5cdfeab37e2320811fb3a71324">applyActivation</a> (<a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> *units, unsigned int units_size, const <a class="el" href="structANNConfiguration.html">ANNConfiguration</a> &amp;conf, bool use_cuda)</td></tr>
<tr class="memdesc:ad9116c5cdfeab37e2320811fb3a71324"><td class="mdescLeft">&#160;</td><td class="mdescRight">Abstract method that activates a "bunch" of units (or neurons).  <a href="#ad9116c5cdfeab37e2320811fb3a71324"></a><br/></td></tr>
<tr class="memitem:a97e7b59c72d8492ca5c6d533065f42ca"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a97e7b59c72d8492ca5c6d533065f42ca">multiplyDerivatives</a> (<a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> *units, <a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> *input_errors, unsigned int <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a68877ab40c24d5c7064e4217f30b77cb">size</a>, const <a class="el" href="structANNConfiguration.html">ANNConfiguration</a> &amp;conf, bool use_cuda, bool is_output)</td></tr>
<tr class="memdesc:a97e7b59c72d8492ca5c6d533065f42ca"><td class="mdescLeft">&#160;</td><td class="mdescRight">Abstract method that computes derivatives.  <a href="#a97e7b59c72d8492ca5c6d533065f42ca"></a><br/></td></tr>
<tr class="memitem:a7183060f94b3fe6f81064ffaa65cc722"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classANN_1_1ActivationFunction.html">ActivationFunction</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a7183060f94b3fe6f81064ffaa65cc722">clone</a> ()</td></tr>
<tr class="memdesc:a7183060f94b3fe6f81064ffaa65cc722"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns a deep copy of the object.  <a href="#a7183060f94b3fe6f81064ffaa65cc722"></a><br/></td></tr>
<tr class="inherit_header pub_methods_classANN_1_1ActivationFunction"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classANN_1_1ActivationFunction')"><img src="closed.png" alt="-"/>&nbsp;Public Member Functions inherited from <a class="el" href="classANN_1_1ActivationFunction.html">ANN::ActivationFunction</a></td></tr>
<tr class="memitem:a423acd015e00692b6d3a4614155805ae inherit pub_methods_classANN_1_1ActivationFunction"><td class="memItemLeft" align="right" valign="top">virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classANN_1_1ActivationFunction.html#a423acd015e00692b6d3a4614155805ae">~ActivationFunction</a> ()</td></tr>
<tr class="inherit_header pub_methods_classReferenced"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classReferenced')"><img src="closed.png" alt="-"/>&nbsp;Public Member Functions inherited from <a class="el" href="classReferenced.html">Referenced</a></td></tr>
<tr class="memitem:ga8654116c264068ee611850bbee8209d4 inherit pub_methods_classReferenced"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__util.html#ga8654116c264068ee611850bbee8209d4">Referenced</a> ()</td></tr>
<tr class="memitem:ga9901ca9b26619914ffeb3c93e2f3b7c3 inherit pub_methods_classReferenced"><td class="memItemLeft" align="right" valign="top">virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__util.html#ga9901ca9b26619914ffeb3c93e2f3b7c3">~Referenced</a> ()</td></tr>
<tr class="memitem:ga6765a42c37ab416e5141530b203e2613 inherit pub_methods_classReferenced"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__util.html#ga6765a42c37ab416e5141530b203e2613">incRef</a> ()</td></tr>
<tr class="memitem:ga5d7de9a6d98891e00084af6b02949dc0 inherit pub_methods_classReferenced"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__util.html#ga5d7de9a6d98891e00084af6b02949dc0">decRef</a> ()</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2><a name="pri-attribs"></a>
Private Attributes</h2></td></tr>
<tr class="memitem:a68877ab40c24d5c7064e4217f30b77cb"><td class="memItemLeft" align="right" valign="top">unsigned int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a68877ab40c24d5c7064e4217f30b77cb">size</a></td></tr>
<tr class="memdesc:a68877ab40c24d5c7064e4217f30b77cb"><td class="mdescLeft">&#160;</td><td class="mdescRight">If use_cuda=true, this is the size of the <a class="el" href="classANN_1_1ActivationUnits.html">ActivationUnits</a>.  <a href="#a68877ab40c24d5c7064e4217f30b77cb"></a><br/></td></tr>
<tr class="memitem:a299bc92420222269db2a5790ece0c533"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a299bc92420222269db2a5790ece0c533">minimums</a></td></tr>
<tr class="memdesc:a299bc92420222269db2a5790ece0c533"><td class="mdescLeft">&#160;</td><td class="mdescRight">Only with use_cuda=true, for map-reduce process.  <a href="#a299bc92420222269db2a5790ece0c533"></a><br/></td></tr>
<tr class="memitem:a7663513fd96ffb98bb400666553865ef"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a7663513fd96ffb98bb400666553865ef">maximums</a></td></tr>
<tr class="memdesc:a7663513fd96ffb98bb400666553865ef"><td class="mdescLeft">&#160;</td><td class="mdescRight">Only with use_cuda=true, for map-reduce process.  <a href="#a7663513fd96ffb98bb400666553865ef"></a><br/></td></tr>
<tr class="memitem:a253b54cd58a503e0f1bf2c4733640014"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a253b54cd58a503e0f1bf2c4733640014">sums</a></td></tr>
<tr class="memdesc:a253b54cd58a503e0f1bf2c4733640014"><td class="mdescLeft">&#160;</td><td class="mdescRight">Only with use_cuda=true, for map-reduce process.  <a href="#a253b54cd58a503e0f1bf2c4733640014"></a><br/></td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2><a name="inherited"></a>
Additional Inherited Members</h2></td></tr>
<tr class="inherit_header pro_attribs_classReferenced"><td colspan="2" onclick="javascript:toggleInherit('pro_attribs_classReferenced')"><img src="closed.png" alt="-"/>&nbsp;Protected Attributes inherited from <a class="el" href="classReferenced.html">Referenced</a></td></tr>
<tr class="memitem:a85b79d29b1b561f765f366d18093585e inherit pro_attribs_classReferenced"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classReferenced.html#a85b79d29b1b561f765f366d18093585e">refs</a></td></tr>
</table>
<a name="details" id="details"></a><h2>Detailed Description</h2>
<div class="textblock"><p>A softmax activation function. </p>
<p>Implements <a class="el" href="classANN_1_1ActivationFunction.html">ActivationFunction</a> interface and, given a bunch_size number of neuron layers, follows the equation:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ a_i = \frac{e^{y_i}}{\sum_{j} e^{y_j}} \]" src="form_2.png"/>
</p>
<p>If use_cuda flag is true, the object reserves memory for the map-reduce process executed at the GPU. Therefore, with use_cuda=true the object is unable to be shared between different <a class="el" href="classANN_1_1ActivationUnits.html">ActivationUnits</a>. </p>
</div><h2>Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="a45abc5e6e6ff36d60d3221be6337ae71"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">ANN::SoftmaxActivationFunction::SoftmaxActivationFunction </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Referenced by <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a7183060f94b3fe6f81064ffaa65cc722">clone()</a>.</p>

</div>
</div>
<a class="anchor" id="a212d8e80f5494a443b184f9f150116b4"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">ANN::SoftmaxActivationFunction::~SoftmaxActivationFunction </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>References <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a7663513fd96ffb98bb400666553865ef">maximums</a>, <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a299bc92420222269db2a5790ece0c533">minimums</a>, and <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a253b54cd58a503e0f1bf2c4733640014">sums</a>.</p>

</div>
</div>
<h2>Member Function Documentation</h2>
<a class="anchor" id="ad9116c5cdfeab37e2320811fb3a71324"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void ANN::SoftmaxActivationFunction::applyActivation </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> *&#160;</td>
          <td class="paramname"><em>units</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">unsigned int&#160;</td>
          <td class="paramname"><em>units_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structANNConfiguration.html">ANNConfiguration</a> &amp;&#160;</td>
          <td class="paramname"><em>conf</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>use_cuda</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Abstract method that activates a "bunch" of units (or neurons). </p>
<p>Abstract method that takes the input neuron potentials and computes the activation:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ a_i = f(y_i | y_1, y_2, \ldots y_n) \]" src="form_3.png"/>
</p>
<p>being <img class="formulaInl" alt="$a_i$" src="form_4.png"/> the activation of neuron <img class="formulaInl" alt="$i$" src="form_5.png"/>, depending on its potential <img class="formulaInl" alt="$y_i$" src="form_6.png"/> and the other neurons potentials.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">units</td><td>an input matrix of size (size X conf.cur_bunch_size). It is stored at an FloatGPUMirroredMemoryBlock object, Each component is a neuron activation.</td></tr>
    <tr><td class="paramname">units_size</td><td>the number of neurons.</td></tr>
    <tr><td class="paramname">conf</td><td>a reference to an <a class="el" href="structANNConfiguration.html">ANNConfiguration</a> struct which contains global parameters as: current bunch size, maximum bunch size, use_cuda global flag, </td></tr>
    <tr><td class="paramname">use_cuda</td><td>a boolean parameter that indicates the use of CPU computation or GPU computation (using CUDA). Overrides the configuration stored at conf parameter. </td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classANN_1_1ActivationFunction.html#a158f2c883f8597d22e67019e9c091ac9">ANN::ActivationFunction</a>.</p>

<p>References <a class="el" href="namespaceapril__utils.html#abcac989f278c71babe68e3620123f1ba">april_utils::ceilingPowerOfTwo()</a>, <a class="el" href="group__math.html#ga67da17e055003f63ff3720cb8394728d">doApplySoftmaxActivation()</a>, <a class="el" href="group__util.html#ga7c141183788078513c128e5f0bb15f48">ERROR_EXIT</a>, <a class="el" href="structANNConfiguration.html#a91fd4386ee095907f82fb33ef2b226ee">ANNConfiguration::max_bunch_size</a>, <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a7663513fd96ffb98bb400666553865ef">maximums</a>, <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a299bc92420222269db2a5790ece0c533">minimums</a>, <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a68877ab40c24d5c7064e4217f30b77cb">size</a>, and <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a253b54cd58a503e0f1bf2c4733640014">sums</a>.</p>

</div>
</div>
<a class="anchor" id="a7183060f94b3fe6f81064ffaa65cc722"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classANN_1_1ActivationFunction.html">ActivationFunction</a> * ANN::SoftmaxActivationFunction::clone </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns a deep copy of the object. </p>

<p>Implements <a class="el" href="classANN_1_1ActivationFunction.html#aa25f18ccc25ab5bc4669eb11631d5c28">ANN::ActivationFunction</a>.</p>

<p>References <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a45abc5e6e6ff36d60d3221be6337ae71">SoftmaxActivationFunction()</a>.</p>

</div>
</div>
<a class="anchor" id="a97e7b59c72d8492ca5c6d533065f42ca"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void ANN::SoftmaxActivationFunction::multiplyDerivatives </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> *&#160;</td>
          <td class="paramname"><em>units</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> *&#160;</td>
          <td class="paramname"><em>input_errors</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">unsigned int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structANNConfiguration.html">ANNConfiguration</a> &amp;&#160;</td>
          <td class="paramname"><em>conf</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>use_cuda</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>is_output</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Abstract method that computes derivatives. </p>
<p>Abstract method that computes the derivative of a "bunch" of units (or neurons) and stores it on input_errors vector. Basically, it computes the following equation:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ \delta_i = \delta_i \cdot \frac{\partial a_i}{\partial y_i} \]" src="form_7.png"/>
</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">units</td><td>an input matrix of size (size X conf.cur_bunch_size). It is stored at an FloatGPUMirroredMemoryBlock object, Each component is a neuron activation.</td></tr>
    <tr><td class="paramname">input_errors</td><td>an input matrix of size (size X conf.cur_bunch_size). It is stored at an FloatGPUMirroredMemoryBlock object. Each component is a sum of error derivatives that comes from the output layer neurons to the current neuron.</td></tr>
    <tr><td class="paramname">size</td><td>an unsigned int parameter indicating the number of neurons.</td></tr>
    <tr><td class="paramname">conf</td><td>an <a class="el" href="structANNConfiguration.html">ANNConfiguration</a> reference which stores some global parameters as: current bunch size, maximum bunch size, use_cuda global flag, ...</td></tr>
    <tr><td class="paramname">use_cuda</td><td>a boolean flag indicating the use of CPU computation or GPU computation (using CUDA). Overrides the configuration stored at conf parameter. </td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classANN_1_1ActivationFunction.html#aea98ce2dca09b0d1aaf1fb3897ad306a">ANN::ActivationFunction</a>.</p>

<p>References <a class="el" href="group__math.html#ga810507190a3d8595b434cbb77e2e8e14">doMultiplyLogisticDerivatives()</a>, and <a class="el" href="structANNConfiguration.html#afa65cbf91202029cc16273cdb3bded5e">ANNConfiguration::error_function_logistic_mandatory</a>.</p>

</div>
</div>
<h2>Member Data Documentation</h2>
<a class="anchor" id="a7663513fd96ffb98bb400666553865ef"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> * ANN::SoftmaxActivationFunction::maximums</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Only with use_cuda=true, for map-reduce process. </p>

<p>Referenced by <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#ad9116c5cdfeab37e2320811fb3a71324">applyActivation()</a>, and <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a212d8e80f5494a443b184f9f150116b4">~SoftmaxActivationFunction()</a>.</p>

</div>
</div>
<a class="anchor" id="a299bc92420222269db2a5790ece0c533"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a>* ANN::SoftmaxActivationFunction::minimums</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Only with use_cuda=true, for map-reduce process. </p>

<p>Referenced by <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#ad9116c5cdfeab37e2320811fb3a71324">applyActivation()</a>, and <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a212d8e80f5494a443b184f9f150116b4">~SoftmaxActivationFunction()</a>.</p>

</div>
</div>
<a class="anchor" id="a68877ab40c24d5c7064e4217f30b77cb"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">unsigned int ANN::SoftmaxActivationFunction::size</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>If use_cuda=true, this is the size of the <a class="el" href="classANN_1_1ActivationUnits.html">ActivationUnits</a>. </p>

<p>Referenced by <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#ad9116c5cdfeab37e2320811fb3a71324">applyActivation()</a>.</p>

</div>
</div>
<a class="anchor" id="a253b54cd58a503e0f1bf2c4733640014"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group__math.html#ga69731d455c34809dc174535f8a5d55c0">FloatGPUMirroredMemoryBlock</a> * ANN::SoftmaxActivationFunction::sums</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Only with use_cuda=true, for map-reduce process. </p>

<p>Referenced by <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#ad9116c5cdfeab37e2320811fb3a71324">applyActivation()</a>, and <a class="el" href="classANN_1_1SoftmaxActivationFunction.html#a212d8e80f5494a443b184f9f150116b4">~SoftmaxActivationFunction()</a>.</p>

</div>
</div>
</div><!-- contents -->

</BODY>
</HTML>
